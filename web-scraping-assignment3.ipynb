{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdf5e9f",
   "metadata": {},
   "source": [
    "9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('c:\\Users\\91832\\Downloads\\output_scraping.csv', 'w', encoding=\"UTF-8\", newline=\"\")\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.writerow(\n",
    "    ['url', 'link_title', 'channel', 'no_of_views', 'time_uploaded', 'comment', 'author', 'comment_posted', \n",
    "     'no_of_replies','upvotes','downvotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_pages = \"https://www.youtube.com/\"\n",
    "locationOfWebdriver = r\"C:\\Users\\91832\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(locationOfWebdriver)\n",
    "driver.get(youtube_pages)\n",
    "time.sleep(10)\n",
    "try:\n",
    "    print(\"=\" * 40)  # Shows in terminal when youtube summary page with search keyword is being scraped\n",
    "    print(\"Scraping \" + youtube_pages)\n",
    "    search = driver.find_element_by_id('search')\n",
    "    search.send_keys(\"Kishore Kumar\")    \n",
    "    driver.find_element_by_id('search-icon-legacy').click()\n",
    "    time.sleep(20)    \n",
    "    vtitle = driver.find_elements_by_id('video-title')\n",
    "    subscription = driver.find_elements_by_id('byline')\n",
    "    views = driver.find_elements_by_xpath('//div[@id=\"metadata-line\"]/span[1]')\n",
    "    posted = driver.find_elements_by_xpath('//div[@id=\"metadata-line\"]/span[2]')\n",
    "    \n",
    "    tcount = 0\n",
    "    href = []\n",
    "    title = []\n",
    "    channel = []\n",
    "    numview = []\n",
    "    postdate = []\n",
    "        \n",
    "    while tcount < 10:\n",
    "        href.append(vtitle[tcount].get_attribute('href'))\n",
    "        channel.append(subscription[tcount].get_attribute('title'))\n",
    "        title.append(vtitle[tcount].text)\n",
    "        numview.append(views[tcount].text)\n",
    "        postdate.append(posted[tcount].text)  \n",
    "        tcount = tcount +1\n",
    "    \n",
    "    # launch top ten extracted links and extract comment details\n",
    "    tcount = 0    \n",
    "    while tcount < 10:  \n",
    "        youtube_dict ={}\n",
    "        # extract comment section of top ten links\n",
    "        url = href[tcount]\n",
    "        print (url)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            print(\"+\" * 40)  # Shows in terminal when details of a new video is being scraped\n",
    "            print(\"Scraping child links \")\n",
    "            #scroll down to load comments\n",
    "            driver.execute_script('window.scrollTo(0,390);')\n",
    "            time.sleep(15)\n",
    "            #sort by top comments\n",
    "            sort= driver.find_element_by_xpath(\"\"\"//*[@id=\"icon-label\"]\"\"\")\n",
    "            sort.click()\n",
    "            time.sleep(10)\n",
    "            topcomments =driver.find_element_by_xpath(\"\"\"//*[@id=\"menu\"]/a[1]/paper-item/paper-item-body/div[1]\"\"\")\n",
    "            topcomments.click()\n",
    "            time.sleep(10)\n",
    "            # Loads 20 comments , scroll two times to load next set of 40 comments. \n",
    "            for i in range(0,2):\n",
    "                driver.execute_script(\"window.scrollTo(0,Math.max(document.documentElement.scrollHeight,document.body.scrollHeight,document.documentElement.clientHeight))\")\n",
    "                time.sleep(10)\n",
    "            \n",
    "            #count total number of comments and set index to number of comments if less than 50 otherwise set as 50. \n",
    "            totalcomments= len(driver.find_elements_by_xpath(\"\"\"//*[@id=\"content-text\"]\"\"\"))\n",
    "         \n",
    "            if totalcomments < 50:\n",
    "                index= totalcomments\n",
    "            else:\n",
    "                index= 50 \n",
    "                \n",
    "            ccount = 0\n",
    "            while ccount < index: \n",
    "                try:\n",
    "                    comment = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')[ccount].text\n",
    "                except:\n",
    "                    comment = \"\"\n",
    "                try:\n",
    "                    authors = driver.find_elements_by_xpath('//a[@id=\"author-text\"]/span')[ccount].text\n",
    "                except:\n",
    "                    authors = \"\"\n",
    "                try:\n",
    "                    comment_posted = driver.find_elements_by_xpath('//*[@id=\"published-time-text\"]/a')[ccount].text\n",
    "                except:\n",
    "                    comment_posted = \"\"\n",
    "                try:\n",
    "                    replies = driver.find_elements_by_xpath('//*[@id=\"more-text\"]')[ccount].text                    \n",
    "                    if replies ==\"View reply\":\n",
    "                        replies= 1\n",
    "                    else:\n",
    "                        replies =replies.replace(\"View \",\"\")\n",
    "                        replies =replies.replace(\" replies\",\"\")\n",
    "                except:\n",
    "                    replies = \"\"\n",
    "                try:\n",
    "                    upvotes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')[ccount].text\n",
    "                except:\n",
    "                    upvotes = \"\"\n",
    "                        \n",
    "                youtube_dict['url'] = href[tcount]\n",
    "                youtube_dict['link_title'] = title[tcount]\n",
    "                youtube_dict['channel'] = channel[tcount]\n",
    "                youtube_dict['no_of_views'] = numview[tcount]\n",
    "                youtube_dict['time_uploaded'] =  postdate[tcount]\n",
    "                youtube_dict['comment'] = comment\n",
    "                youtube_dict['author'] = authors\n",
    "                youtube_dict['comment_posted'] = comment_posted\n",
    "                youtube_dict['no_of_replies'] = replies\n",
    "                youtube_dict['upvotes'] = upvotes\n",
    "                \n",
    "                writer.writerow(youtube_dict.values())\n",
    "                ccount = ccount +1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            driver.close()\n",
    "        tcount = tcount +1 \n",
    "    print(\"Scrapping process Completed\")   \n",
    "    csv_file.close()    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6594622",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selectorlib import Extractor\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "def search_amazon(item):\n",
    "\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get('https://www.amazon.com')\n",
    "    search_box = driver.find_element_by_id('twotabsearchtextbox').send_keys(item)\n",
    "    search_button = driver.find_element_by_id(\"nav-search-submit-text\").click()\n",
    "\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    try:\n",
    "        num_page = driver.find_element_by_xpath('//*[@class=\"a-pagination\"]/li[6]')\n",
    "    except NoSuchElementException:\n",
    "        num_page = driver.find_element_by_class_name('a-last').click()\n",
    "\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    url_list = []\n",
    "\n",
    "    for i in range(int(num_page.text)):\n",
    "        page_ = i + 1\n",
    "        url_list.append(driver.current_url)\n",
    "        driver.implicitly_wait(4)\n",
    "        click_next = driver.find_element_by_class_name('a-last').click()\n",
    "        print(\"Page \" + str(page_) + \" grabbed\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "    with open('search_results_urls.txt', 'w') as filehandle:\n",
    "        for result_page in url_list:\n",
    "            filehandle.write('%s\\n' % result_page)\n",
    "\n",
    "    print(\"---DONE---\")\n",
    "\n",
    "def scrape(url):\n",
    "\n",
    "    headers = {\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'referer': 'https://www.amazon.com/',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "    }\n",
    "\n",
    "    # Download the page using requests\n",
    "    print(\"Downloading %s\"%url)\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # Simple check to check if page was blocked (Usually 503)\n",
    "    if r.status_code > 500:\n",
    "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "            print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%url)\n",
    "        else:\n",
    "            print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(url,r.status_code))\n",
    "        return None\n",
    "    # Pass the HTML of the page and create\n",
    "    return e.extract(r.text)\n",
    "\n",
    "search_amazon('Macbook Pro') # <------ search query goes here.\n",
    "\n",
    "# Create an Extractor by reading from the YAML file\n",
    "e = Extractor.from_yaml_file('search_results.yml')\n",
    "\n",
    "# product_data = []\n",
    "with open(\"search_results_urls.txt\",'r') as urllist, open('search_results_output.jsonl','w') as outfile:\n",
    "    for url in urllist.read().splitlines():\n",
    "        data = scrape(url)\n",
    "        if data:\n",
    "            for product in data['products']:\n",
    "                product['search_url'] = url\n",
    "                print(\"Saving Product: %s\"%product['title'].encode('utf8'))\n",
    "                json.dump(product,outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "                # sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b05fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
